{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8N-9-2H8Yy9"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "!pip install pyngrok -q\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!pip install fastapi[all] -q\n",
        "!pip install uvicorn[standard] -q\n",
        "!pip install python-multipart -q\n",
        "!pip install nest-asyncio -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.DEBUG, force=True)"
      ],
      "metadata": {
        "id": "IgQe6k6X8Yc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Required Libraries"
      ],
      "metadata": {
        "id": "Pw3wFHXf86vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import sys\n",
        "\n",
        "USE_GPU = True #@param {\"type\": \"boolean\"}\n",
        "CPU_ARCH = platform.machine()\n",
        "OS_TYPE = sys.platform\n",
        "\n",
        "logging.debug(f'CPU Arch: {CPU_ARCH}')\n",
        "logging.debug(f'OS: {OS_TYPE}')\n",
        "\n",
        "if (OS_TYPE == 'win' or OS_TYPE == 'linux') and CPU_ARCH == 'x86_64':\n",
        "  CPU_ARCH = 'x64'\n",
        "\n",
        "ONNX_VERSION = \"1.13.1\"\n",
        "ONNX_VAR = f\"{CPU_ARCH}{'-gpu' if USE_GPU else ''}\"\n",
        "ONNX_NAME = f\"onnxruntime-{OS_TYPE}-{ONNX_VAR}-{ONNX_VERSION}\"\n",
        "ONNX_URL = f'https://github.com/microsoft/onnxruntime/releases/download/v{ONNX_VERSION}/{ONNX_NAME}.tgz'\n",
        "\n",
        "logging.debug(f'ONNX Filename = {ONNX_NAME}')\n",
        "!wget -N $ONNX_URL\n",
        "!tar -xzf {ONNX_NAME}.tgz\n",
        "\n",
        "!cp /content/{ONNX_NAME}/lib/* /usr/local/lib\n",
        "!cp /content/{ONNX_NAME}/lib/* /content"
      ],
      "metadata": {
        "id": "fg2P_I848vJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOICEVOX_VERSION = '0.14.2'\n",
        "VOICEVOX_FILE = f\"voicevox_core-{VOICEVOX_VERSION}+{'cuda' if USE_GPU else 'cpu'}-cp38-abi3-{OS_TYPE}_{platform.machine()}.whl\"\n",
        "\n",
        "!pip install https://github.com/VOICEVOX/voicevox_core/releases/download/{VOICEVOX_VERSION}/{VOICEVOX_FILE}"
      ],
      "metadata": {
        "id": "0zT1QrOTFCpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPEN_JTALK_NAME = \"open_jtalk_dic_utf_8-1.11\"\n",
        "\n",
        "!wget -N https://onboardcloud.dl.sourceforge.net/project/open-jtalk/Dictionary/open_jtalk_dic-1.11/{OPEN_JTALK_NAME}.tar.gz\n",
        "!tar -xzf {OPEN_JTALK_NAME}.tar.gz\n",
        "\n",
        "OPEN_JTALK_DIR = f\"/content/{OPEN_JTALK_NAME}\""
      ],
      "metadata": {
        "id": "B-yCYu3oEIs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ],
      "metadata": {
        "id": "GiHG4gsuF_Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from pyngrok import ngrok\n",
        "from voicevox_core import AccelerationMode, AudioQuery, VoicevoxCore\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"\" #@param {type:\"string\"}\n",
        "TRANSLATE_FILENAME = 'translate.wav' #@param {type:\"string\"}\n",
        "TRANSCRIBE_FILENAME = 'transcribe.wav' #@param {type:\"string\"}\n",
        "WHISPER_MODEL = \"small\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
        "CHUNK_SIZE = 4096 #@param {type:\"integer\"}\n",
        "DEFAULT_VOICE_ID = 5 #@param {type:\"integer\"}\n",
        "\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Whisper\n",
        "logging.info(f\"[WHISPER] loading up \\\"{WHISPER_MODEL}\\\" model..\")\n",
        "model = whisper.load_model(WHISPER_MODEL)\n",
        "logging.info(f\"[WHISPER] loaded\")\n",
        "\n",
        "# VoiceVox\n",
        "logging.info(f\"[VOICEVOX] loading up voicevox core..\")\n",
        "\n",
        "core = VoicevoxCore(\n",
        "    acceleration_mode=\"GPU\" if USE_GPU else \"CPU\", open_jtalk_dict_dir=OPEN_JTALK_DIR\n",
        ")\n",
        "core.load_model(DEFAULT_VOICE_ID)\n",
        "\n",
        "logging.info(f\"[VOICEVOX] successfully loaded! running on {'gpu' if core.is_gpu_mode else 'cpu'}\")"
      ],
      "metadata": {
        "id": "CcnDMFxKc7qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "import json\n",
        "from fastapi import FastAPI, File, UploadFile, Request, Response, Body\n",
        "from typing import Annotated, Union, Dict, Any\n",
        "import asyncio\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "app = FastAPI()\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "async def main():\n",
        "  return { 'message': 'Hello world' }\n",
        "\n",
        "# TTS\n",
        "@app.post('/audio_query')\n",
        "def query(text: str, speaker: int = DEFAULT_VOICE_ID):\n",
        "  start = time.time()\n",
        "\n",
        "  if not core.is_model_loaded(speaker):\n",
        "    core.load_model(speaker)\n",
        "\n",
        "  logging.debug(\"querying voicevox\")\n",
        "\n",
        "  audio_query = core.audio_query(text, speaker)\n",
        "  audio_query.output_stereo = True\n",
        "\n",
        "  logging.debug(f\"querying took: {time.time() - start}\")\n",
        "\n",
        "  return audio_query\n",
        "\n",
        "@app.post('/synthesis')\n",
        "def synthesis(audio_query: Annotated[AudioQuery, Body()], speaker: int = DEFAULT_VOICE_ID):\n",
        "  start = time.time()\n",
        "\n",
        "  if not core.is_model_loaded(speaker):\n",
        "    core.load_model(speaker)\n",
        "\n",
        "  logging.debug('synthesizing')\n",
        "\n",
        "  wav = core.synthesis(audio_query, speaker)\n",
        "\n",
        "  logging.debug(f\"synthesizing took: {time.time() - start}\")\n",
        "\n",
        "  return Response(content=wav, media_type=\"audio/wav\")\n",
        "\n",
        "@app.post('/tts')\n",
        "def query(\n",
        "    text: str,\n",
        "    speaker: int = DEFAULT_VOICE_ID,\n",
        "\n",
        "    speed_scale: float = 1.7,\n",
        "    volume_scale: float = 2.0,\n",
        "    intonation_scale: float = 1.5,\n",
        "    pre_phoneme_length: float = 1.0,\n",
        "    post_phoneme_length = 1.0\n",
        "):\n",
        "  start = time.time()\n",
        "\n",
        "  if not core.is_model_loaded(speaker):\n",
        "    core.load_model(speaker)\n",
        "\n",
        "  logging.debug(\"querying voicevox\")\n",
        "\n",
        "  audio_query = core.audio_query(text, speaker)\n",
        "  audio_query.output_stereo = True\n",
        "  audio_query.speed_scale = float(speed_scale)\n",
        "  audio_query.volume_scale = float(volume_scale)\n",
        "  audio_query.intonation_scale = float(intonation_scale)\n",
        "  audio_query.pre_phoneme_length = float(pre_phoneme_length)\n",
        "  audio_query.post_phoneme_length = float(post_phoneme_length)\n",
        "\n",
        "  logging.debug(f\"querying took: {time.time() - start}\")\n",
        "\n",
        "  logging.debug('synthesizing')\n",
        "\n",
        "  wav = core.synthesis(audio_query, speaker)\n",
        "\n",
        "  logging.debug(f\"synthesizing took: {time.time() - start}\")\n",
        "\n",
        "  return Response(content=wav, media_type=\"audio/wav\")\n",
        "\n",
        "# STT\n",
        "@app.post('/asr')\n",
        "def asr(audio_file: UploadFile, task: str = 'transcribe', language: str = 'ja'):\n",
        "  if audio_file.size <= 0:\n",
        "    return JSONResponse(content={ 'message': 'Missing audio' }, status_code=422)\n",
        "\n",
        "  if task == 'transcribe':\n",
        "    with open(TRANSCRIBE_FILENAME, 'wb') as f:\n",
        "      audio_file.file.seek(0)\n",
        "      f.write(audio_file.file.read())\n",
        "\n",
        "    result = model.transcribe(TRANSCRIBE_FILENAME)\n",
        "\n",
        "    return result\n",
        "\n",
        "  elif task == 'translate':\n",
        "    with open(TRANSLATE_FILENAME, 'wb') as f:\n",
        "      audio_file.file.seek(0)\n",
        "      f.write(audio_file.file.read())\n",
        "\n",
        "    result = model.transcribe(TRANSLATE_FILENAME, language=language, task='translate')\n",
        "\n",
        "    return result\n",
        "  \n",
        "  else:\n",
        "    return JSONResponse(content = { 'message': 'Bad request' }, status_code=400)\n",
        "\n",
        "@app.post('/asr_stream')\n",
        "async def asr_stream(req: Request, task: str = 'transcribe', language: str = 'ja'):\n",
        "  audio_data = BytesIO()\n",
        "\n",
        "  async for chunk in req.stream():\n",
        "    audio_data.write(chunk)\n",
        "\n",
        "  audio = np.frombuffer(audio_data.getvalue(), np.int16).flatten().astype(np.float32) / 32768.0\n",
        "  audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "  if task == 'transcribe':\n",
        "    result = model.transcribe(audio)\n",
        "\n",
        "    return result\n",
        "\n",
        "  elif task == 'translate':\n",
        "    result = model.transcribe(audio, language=language, task='translate')\n",
        "\n",
        "    return result\n",
        "  \n",
        "  else:\n",
        "    return JSONResponse(content = { 'message': 'Bad request' }, status_code=400)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  http_tunnel = ngrok.connect(5000)\n",
        "  print(f'Public URL -> {http_tunnel.public_url}')\n",
        "  print(f'Docs URL -> {http_tunnel.public_url}/docs')\n",
        "\n",
        "  nest_asyncio.apply()\n",
        "  uvicorn.run(app, port=5000)"
      ],
      "metadata": {
        "id": "9AaXP5RHewF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}